# Специфікація проекту "FalkorDB MCP Сервер"

## 1. Коротке РЕЗЮМЕ (Executive Summary)
Ми створюємо MCP-сервер (Model Context Protocol) на базі Python (`falkordb-service/mcp`) всередині Docker-контейнера для взаємодії з графовою базою `FalkorDB`. Основна мета — надати мені (Агенту-Оркестратору) та майбутньому рушію (Gemini CLI) прямі, гранульовані і надійні інструменти (Tools) для запису та читання графової пам'яті через транспорт SSE поверх HTTP.

## 2. Опис Проблеми
Існуючий метод запису у граф (`memory_bridge.py` через генерацію файлів, `docker cp` та `docker exec`) є громіздким, крихким до проблем з кодуванням та забруднює контекстне вікно агента логами bash-команд. Це знижує автономність агента та з'їдає ресурси на рутину замість стратегічного планування.

## 3. Критерії Успіху
- Агент та Gemini CLI можуть взаємодіяти з FalkorDB без будь-яких `run_command` (тільки нативні виклики Tool-ів).
- Асинхронними викликами інструментів та їх серійністю повністю керує Агент-Оркестратор (відсутність "split-brain" і станів гонки при записі в базу).
- Контейнер MCP-сервера стабільно тримає SSE-з'єднання у межах `grynya-net`.

## 4. Портрети Користувачів (User Personas)
* **Grynya (Я):** LLM-агент, що веде сесії, аналізує розмови, зберігає контекст та викликає інструменти в межах свого контекстного вікна.
* **Gemini CLI:** Майбутній сервісний CLI-агент, який автономно парситиме сутності з текстів розмов та виконуватиме складні графові запити (Smart Search).

## 5. Шлях Користувача (User Journey Flow)
1. Користувач починає розмову з триггеру `/db`, `/sa` або `/ss`.
2. Агент (або Gemini) викликає зовнішній MCP Tool (наприклад, `add_knowledge_node(data)`).
3. JSON-параметри летять по SSE до MCP-сервера (FastAPI/Starlette).
4. Сервер виконує Cypher-запит до `FalkorDB` (у цьому ж Docker network) за допомогою бібліотеки `falkordb`.
5. Сервер віддає статус-відповідь назад агенту.

## 6. Функціональні Вимоги

### Обов'язково зробити (P0 - Must Have)
- Реалізувати SSE-транспорт на базі `mcp-sdk` поверх HTTP-фреймворку (FastAPI або Starlette).
- Додати інструмент `create_session` (ініціалізація або відкриття існуючої сесії з оновленням Time/Day).
- Додати інструменти для створення гранульованих вузлів: `add_node` / `link_nodes`.
- Додати інструмент `query_graph` для читання даних з FalkorDB (Cypher read-only query).

### Бажано зробити (P1 - Should Have)
- Додати інструмент `update_last_event`, щоб Агент-Оркестратор оберігав ланцюжок і вручну оновлював вказівник сесії.
- Додати схему валідації (Pydantic models) для вхідних аргументів MCP Tools згідно зі схемою `grynya-schema.md`.

### Було б чудово (P2 - Nice to Have)
- Вбудована генерація складних Cypher-запитів на базі простих JSON-фільтрів.
- Окремий ендпоінт для "ping" та healthcheck.

## 7. Технічна Архітектура

### Модель Даних (Data Model)
Всі сутності, вузли та зв'язки жорстко відповідають `grynya-schema.md` (Session, Request, Response, Analysis, Feedback, Entity). Схема не міняється, проте інструменти взаємодії з нею стають атомарними (гранульованими).

### Компоненти Системи
1. **FalkorDB Container:** Сама графова БД.
2. **MCP Server Container (python):** FastAPI застосунок з `mcp-sdk`, що тримає з'єднання і ходить до FalkorDB через python-драйвер `falkordb`. Замінює поточний `grynya-bridge` або базується в ньому (за умови відкриття порту).
3. **LLM Client / Gemini CLI:** Трейсери та клієнти в тій же мережі (або з доступом до мережі `grynya-net`).

### Інтеграції (External APIs)
Даний сервер не залежить від зовнішніх SaaS-продуктів. Він комунікує лише з LLM-клієнтами (через SSE) та з Redis/FalkorDB (через TCP: 6379).

### Модель Безпеки та Авторизації
Сервер знаходитиметься всередині ізольованої docke-мережі (`grynya-net`), доступ до якої мають виключно інші контейнери (в т.ч. Gemini CLI та внутрішній локальний клієнт через проброс, якщо піднято спільний localhost). Зовнішня аутентифікація не потрібна (trust within network).

## 8. Нефункціональні Вимоги
- **Продуктивність:** Сервер повинен тримати асинхронні SSE з'єднання без блокувань (async/await).
- **Масштабування:** Сервер повинен вміти приймати десятки послідовних/паралельних викликів, але гарантія послідовності (linearizability) для графового ланцюга лежить клієнті.
- **Надійність:** Restart policies у `docker-compose` (unless-stopped) для відновлення з'єднання при збоях БД.

## 9. Що "Поза рамками" (Out of Scope)
- Автоматична "розумна" або "роутингова" черга синхронізації `LAST_EVENT` (lock/mutex) на самому MCP-сервері (це сфера відповідальності клієнта/Оркестратора).
- Генерація складного природного тексту чи LLM-виклик всередині нашого MCP-серверу (він лише DB Client Wrapper).

## 10. Відкриті питання для опрацювання розробниками
- Конкретна структура портів у `docker-compose.yml`: Чи будемо "ховати" HTTP/SSE порт повністю в `grynya-net`, чи мапляти на `localhost:8000` для доступу з IDE (Claude Desktop config)? Якщо це лише для Gemini CLI у цьому ж Docker — порт зовні не потрібен. Оскільки є IDE (тут), можливо, треба відкрити.
- Вибір фреймворку: FastAPI чи Starlette (стандартна практика для `mcp.server.sse`).
